{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import pylab\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import shapiro, normaltest\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Local imports\n",
    "from local_models import *\n",
    "from helper_functions import *\n",
    "from piece_hurdle_model import *\n",
    "from optimize_explanations import *\n",
    "from evaluation_metrics import *\n",
    "\n",
    "from IPython.display import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_dataloaders():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.FashionMNIST(\n",
    "        root='./data/after_anon_review',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_set = torchvision.datasets.FashionMNIST(\n",
    "        root='./data/after_anon_review',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, cnn = load_models(CNN, Generator)\n",
    "\n",
    "#train_loader, test_loader = load_dataloaders()\n",
    "train_loader, test_loader = load_fashion_dataloaders()\n",
    "X_train, y_train, X_test, y_test = get_MNIST_data(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FashionMNIST_data(datasets):\n",
    "    fashion_mnist_trainset = datasets.FashionMNIST(root='./data/fashion_mnist_train', train=True, download=True, transform=None)\n",
    "    fashion_mnist_testset = datasets.FashionMNIST(root='./data/fashion_mnist_test', train=False, download=True, transform=None)\n",
    "    X_train = fashion_mnist_trainset.data\n",
    "    y_train = fashion_mnist_trainset.targets\n",
    "    X_test = fashion_mnist_testset.data\n",
    "    y_test = fashion_mnist_testset.targets\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_feature_contribution_data(data_loader, cnn, num_classes=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Return feature contribution data for each class based on the given data loader and CNN model.\n",
    "    \n",
    "    Args:\n",
    "    - data_loader: DataLoader object containing the dataset to analyze.\n",
    "    - cnn: CNN model used for feature extraction.\n",
    "    - num_classes: Number of classes in the dataset. Default is 10.\n",
    "    \n",
    "    Returns:\n",
    "    - pred_idx: A dictionary containing feature contribution data for each class.\n",
    "                Keys represent class names, and values are lists of feature contributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_data = dict()\n",
    "    pred_idx = dict() \n",
    "\n",
    "    for class_name in list(range(num_classes)):\n",
    "        pred_idx[class_name] = list()\n",
    "        \n",
    "    for i, data in enumerate(data_loader):\n",
    "        # print progress\n",
    "        if i % 10000 == 0:\n",
    "            print(  100 * round(i / len(data_loader), 2), \"% complete...\"  )     \n",
    "        image, label = data\n",
    "        label = int(label.detach().numpy())\n",
    "        acts = cnn(image)[1][0].detach().numpy()\n",
    "        pred = int(torch.argmax(  cnn(image)[0]  ).detach().numpy()) \n",
    "        pred_idx[pred].append(acts.tolist())\n",
    "                \n",
    "    return pred_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13478\\AppData\\Local\\Temp\\ipykernel_18932\\2353536552.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  label = int(label.detach().numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0 % complete...\n",
      "33.0 % complete...\n",
      "50.0 % complete...\n",
      "67.0 % complete...\n",
      "83.0 % complete...\n"
     ]
    }
   ],
   "source": [
    "# collected_data is a dictionary where each key represents a class, and the corresponding value is a list containing activations for that class.\n",
    "collected_data = return_feature_contribution_data(train_loader, cnn)\n",
    "dist_data = {}\n",
    "\n",
    "# Assuming num_classes is the number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# Create an empty list for each class\n",
    "for class_name in range(num_classes):\n",
    "    dist_data[class_name] = {'activations': []}\n",
    "\n",
    "# Fill data from pred_idx_train into dist_data\n",
    "for class_name, activations_list in collected_data.items():\n",
    "    # Convert activations_list to a numpy array\n",
    "    activations_array = np.array(activations_list)\n",
    "    # Store activations_array into the corresponding class in dist_data\n",
    "    dist_data[class_name]['activations'] = activations_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dist_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollected_fashion.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m----> 2\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mdist_data\u001b[49m, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave pickle \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dist_data' is not defined"
     ]
    }
   ],
   "source": [
    "with open('collected_fashion.pickle', 'wb') as handle:\n",
    "    pickle.dump(dist_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"save pickle \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
